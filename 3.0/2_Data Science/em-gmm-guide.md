# EM, GMM 指南

## Jensen 不等式

若 f 是凸函数，基本 Jensen 不等式

$$
f(\theta x+(1-\theta)y \le \theta f(x)+(1-\theta)f(y)
$$

若 $\theta_1,\dots,\theta_k \ge 0\;,\;\theta_1+\dots+\theta_k=1$，则

$$
f(\theta_1 x_1+\dots+\theta_k x_k) \le \theta_1 f(x_1)+\dots+\theta_k f(x_k)
$$

且

$$
f(\mathbf{E}x) \le \mathbf{E}f(x)
$$

## 考察高斯分布

若给定一组样本 $x_1,x_2,\dots,x_n$，已知它们来自于高斯分布 $N(\mu,\sigma)$，试估计参数 $\mu,\sigma$

高斯分布的概率密度函数

$$
f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

将 $x_1,x_2,\dots,x_n$ 带入，得到样本的似然函数

$$
L(x)=\prod_i \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}
$$

化简对数似然函数

$$
\begin{align*} 
l(x)&=log\prod_i \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}} \\
&=\sum_ilog \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}} \\
&=\left(\sum_i log\frac{1}{\sqrt{2\pi}\sigma}\right)+\left( \sum_i-\frac{(x_i-\mu)^2}{2\sigma^2}\right) \\
&=-\frac{n}{2}log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_i(x_i-\mu)^2
\end{align*} 
$$

可得目标函数

$$
l(x)= -\frac{n}{2}log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_i(x_i-\mu)^2
$$

将目标函数对参数 $\mu,\sigma$分别求偏导，可得

$$
\mu=\frac{1}{n}\sum_ix_i \\
\sigma^2=\frac{1}{n}\sum_i(x_i-\mu)^2
$$

## 直观理解 GMM 参数估计

假设我们需要调查我们学校的男生和女生的身高分布。假设你在校园里随便地活捉了100个男生和100个女生。他们共200个人（也就是200个身高的样本数据，为了方便表示，下面，我说“人”的意思就是对应的身高）。样本中存在男性和女性，它们服从 $N(\mu_1,\sigma_1)$ 和 $N(\mu_2,\sigma_2)$ 的分布，试估计 $\mu_1,\sigma_1,\mu_2,\sigma_2$，这里我们不知道性别，对于存在一个隐变量的分布(这里就是不知道性别)，用 EM 算法就非常合适

假如我们知道性别的话，用数学的语言来说就是：在学校那么多男生（身高）中，我们独立地按照概率密度$p(x\;|\;\theta)$抽取100了个（身高），组成样本集 X，我们想通过样本集 X 来估计出未知参数 $\theta$。这里概率密度$p(x\;|\;\theta)$我们知道了是高斯分布$N(\mu,\sigma)$的形式，其中的未知参数是$\theta=[\mu,\sigma]^T$。抽到的样本集是$X=\{x_1,x_2,\dots,x_N\}$，其中$x_i$表示抽到的第 i 个人的身高，这里 N 就是100，表示抽到的样本个数。

由于每个样本都是独立地从$p(x\;|\;\theta)$中抽取的，换句话说这100个男生中的任何一个，都是我随便捉的，从我的角度来看这些男生之间是没有关系的。那么，我从学校那么多男生中为什么就恰好抽到了这100个人呢？抽到这100个人的概率是多少呢？因为这些男生（的身高）是服从同一个高斯分布$p(x\;|\;\theta)$的。那么我抽到男生A（的身高）的概率是$p(x_A\;|\;\theta)$，抽到男生B的概率是$p(x_B\;|\;\theta)$，那因为他们是独立的，所以很明显，我同时抽到男生A和男生B的概率是$p(x_A\;|\;\theta)\times p(x_B\;|\;\theta)$，同理，我同时抽到这100个男生的概率就是他们各自概率的乘积了。用数学家的口吻说就是从分布是$p(x\;|\;\theta)$的总体样本中抽取到这100个样本的概率，也就是样本集X中各个样本的联合概率，用下式表示：

$$
L(x)=L(x_1,\dots,x_n;\theta)=\prod_i \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}
$$

这个概率反映了，在概率密度函数的参数是$\theta$时，得到 X 这组样本的概率。因为这里 X 是已知的，也就是说我抽取到的这100个人的身高可以测出来，也就是已知的了。而$\theta$是未知了，则上面这个公式只有$\theta$是未知数，所以它是$\theta$的函数。这个函数放映的是在不同的参数$\theta$取值下，取得当前这个样本集的可能性，因此称为参数$\theta$相对于样本集 X 的似然函数（likehood function）。记为$L(\theta)$。

在学校那么男生中，我一抽就抽到这100个男生（表示身高），而不是其他人，那是不是表示在整个学校中，这100个人（的身高）出现的概率最大啊。那么这个概率怎么表示？哦，就是上面那个似然函数$L(\theta)$。所以，我们就只需要找到一个参数$\theta$，其对应的似然函数$L(\theta)$最大，也就是说抽到这100个男生（的身高）概率最大。这个叫做$\theta$的最大似然估计量，记为：

$$
\hat{\theta}=arg\;max\;l(\theta)
$$

有时，可以看到$L(\theta)$是连乘的，所以为了便于分析，还可以定义对数似然函数，将其变成连加的：

$$
H(\theta)=ln\;L(\theta)=ln\prod_{i=1}^np(x_i;\theta)=\sum_{i=1}^nln\;p(x_i;\theta)
$$

好了，现在我们知道了，要求$\theta$，只需要使$\theta$的似然函数$L(\theta)$极大化，然后极大值对应的$\theta$就是我们的估计。这里就回到了求最值的问题了。怎么求一个函数的最值？当然是求导，然后让导数为0，那么解这个方程得到的$\theta$就是了（当然，前提是函数$L(\theta)$连续可微）。那如果$\theta$是包含多个参数的向量那怎么处理啊？当然是求$L(\theta)$对所有参数的偏导数，也就是梯度了，那么 n 个未知的参数，就有 n 个方程，方程组的解就是似然函数的极值点了，当然就得到这 n 个参数了。

最大似然估计你可以把它看作是一个反推。多数情况下我们是根据已知条件来推算结果，而最大似然估计是已经知道了结果，然后寻求使该结果出现的可能性最大的条件，以此作为估计值。

极大似然估计，只是一种概率论在统计学的应用，它是参数估计的方法之一。说的是已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，参数估计就是通过若干次试验，观察其结果，利用结果推出参数的大概值。最大似然估计是建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数作为估计的真实值。

求最大似然函数估计值的一般步骤：

1. 写出似然函数；
2. 对似然函数取对数，并整理；
3. 求导数，令导数为0，得到似然方程；
4. 解似然方程，得到的参数即为所求；

再回到例子本身，如果没有“男的左边，女的右边，其他的站中间！”这个步骤，或者说我抽到这200个人中，某些男生和某些女生一见钟情，已经好上了，纠缠起来了。咱们也不想那么残忍，硬把他们拉扯开。那现在这200个人已经混到一起了，这时候，你从这200个人（的身高）里面随便给我指一个人（的身高），我都无法确定这个人（的身高）是男生（的身高）还是女生（的身高）。也就是说你不知道抽取的那200个人里面的每一个人到底是从男生的那个身高分布里面抽取的，还是女生的那个身高分布抽取的。用数学的语言就是，抽取得到的每个样本都不知道是从哪个分布抽取的。

这个时候，对于每一个样本或者你抽取到的人，就有两个东西需要猜测或者估计的了，一是这个人是男的还是女的？二是男生和女生对应的身高的高斯分布的参数是多少？

只有当我们知道了哪些人属于同一个高斯分布的时候，我们才能够对这个分布的参数作出靠谱的预测，例如刚开始的最大似然所说的，但现在两种高斯分布的人混在一块了，我们又不知道哪些人属于第一个高斯分布，哪些属于第二个，所以就没法估计这两个分布的参数。反过来，只有当我们对这两个分布的参数作出了准确的估计的时候，才能知道到底哪些人属于第一个分布，那些人属于第二个分布。

这就成了一个先有鸡还是先有蛋的问题了。鸡说，没有我，谁把你生出来的啊。蛋不服，说，没有我，你从哪蹦出来啊。（呵呵，这是一个哲学问题。当然了，后来科学家说先有蛋，因为鸡蛋是鸟蛋进化的）。为了解决这个你依赖我，我依赖你的循环依赖问题，总得有一方要先打破僵局，说，不管了，我先随便整一个值出来，看你怎么变，然后我再根据你的变化调整我的变化，然后如此迭代着不断互相推导，最终就会收敛到一个解。这就是EM算法的基本思想了。

例如，小时候，老妈给一大袋糖果给你，叫你和你姐姐等分，然后你懒得去点糖果的个数，所以你也就不知道每个人到底该分多少个。咱们一般怎么做呢？先把一袋糖果目测的分为两袋，然后把两袋糖果拿在左右手，看哪个重，如果右手重，那很明显右手这代糖果多了，然后你再在右手这袋糖果中抓一把放到左手这袋，然后再感受下哪个重，然后再从重的那袋抓一小把放进轻的那一袋，继续下去，直到你感觉两袋糖果差不多相等了为止。呵呵，然后为了体现公平，你还让你姐姐先选了。

EM算法就是这样，假设我们想估计知道A和B两个参数，在开始状态下二者都是未知的，但如果知道了A的信息就可以得到B的信息，反过来知道了B也就得到了A。可以考虑首先赋予A某种初值，以此得到B的估计值，然后从B的当前值出发，重新估计A的取值，这个过程一直持续到收敛为止。

EM的意思是“Expectation Maximization”，在我们上面这个问题里面，我们是先随便猜一下男生（身高）的正态分布的参数：如均值和方差是多少。例如男生的均值是1米7，方差是0.1米（当然了，刚开始肯定没那么准），然后计算出每个人更可能属于第一个还是第二个正态分布中的（例如，这个人的身高是1米8，那很明显，他最大可能属于男生的那个分布），这个是属于Expectation一步。有了每个人的归属，或者说我们已经大概地按上面的方法将这200个人分为男生和女生两部分，我们就可以根据之前说的最大似然那样，通过这些被大概分为男生的n个人来重新估计第一个分布的参数，女生的那个分布同样方法重新估计。这个是Maximization。然后，当我们更新了这两个分布的时候，每一个属于这两个分布的概率又变了，那么我们就再需要调整E步……如此往复，直到参数基本不再发生变化为止。

这里把每个人（样本）的完整描述看做是三元组$y_i=\{x_i,z_{i1},z_{i2}\}$，其中，$x_i$是第 i 个样本的观测值，也就是对应的这个人的身高，是可以观测到的值。$z_{i1}$和$z_{i2}$表示男生和女生这两个高斯分布中哪个被用来产生值$x_i$，就是说这两个值标记这个人到底是男生还是女生（的身高分布产生的）。这两个值我们是不知道的，是隐含变量。确切的说，$z_{ij}$在$x_i$由第 j 个高斯分布产生时值为1，否则为0。例如一个样本的观测值为1.8，然后他来自男生的那个高斯分布，那么我们可以将这个样本表示为{1.8, 1, 0}。如果$z_{i1}$和$z_{i2}$的值已知，也就是说每个人我已经标记为男生或者女生了，那么我们就可以利用上面说的最大似然算法来估计他们各自高斯分布的参数。但是它们未知，因此我们只能用EM算法。

咱们现在不是因为那个恶心的隐含变量（抽取得到的每个样本都不知道是从哪个分布抽取的）使得本来简单的可以求解的问题变复杂了，求解不了吗。那怎么办呢？人类解决问题的思路都是想能否把复杂的问题简单化。好，那么现在把这个复杂的问题逆回来，我假设已经知道这个隐含变量了，哎，那么求解那个分布的参数是不是很容易了，直接按上面说的最大似然估计就好了。那你就问我了，这个隐含变量是未知的，你怎么就来一个假设说已知呢？你这种假设是没有根据的。呵呵，我知道，所以我们可以先给这个给分布弄一个初始值，然后求这个隐含变量的期望，当成是这个隐含变量的已知值，那么现在就可以用最大似然求解那个分布的参数了吧，那假设这个参数比之前的那个随机的参数要好，它更能表达真实的分布，那么我们再通过这个参数确定的分布去求这个隐含变量的期望，然后再最大化，得到另一个更优的参数，……迭代，就能得到一个皆大欢喜的结果了。

用数学的方式来描述就是：

随机变量 X 是有 K 个高斯分布混合而成，取各个高斯分布的概率为 $\pi_1,\pi_2,\dots,\pi_K$，第 i 个高斯分布的均值为 $\mu_1$，方差为 $\sum_i$。若观测到随机变量 X 的一系列样本 $x_1,x_2,\dots,x_n$，试估计参数 $\pi,\mu,\Sigma$

建立目标函数，产生这个样本的对数似然函数为

$$
l(x)=\sum_{i=1}^{N}log\left(\sum_{k=1}^{K}\pi_kN(x_i\;|\;\mu_k,\Sigma_k)\right)
$$

由于在对数函数里面又有求和，没办法用直接求导解方程的办法直接求得极大值。为此，我们分成两步解决这个问题。

### 第一步 估算数据来自哪个组份

对于每个数据 $x_i$ 来说，它由第 k 个组份生成的概率为

$$
\gamma(i,k)=\frac{\pi_kN(x_i\;|\;\mu_k,\Sigma_k)}{\sum_{j=1}^K\pi_jN(x_i\;|\;\mu_j,\Sigma_j)}
$$

由于式子里 $\mu,\Sigma$ 也是需要我们估计的值，我们采用迭代发，在计算 $\gamma(i,k)$ 的时候假定 $\mu,\Sigma$ 均已知。第一次计算时，需要先验知识给定 $\mu,\Sigma$。

### 第二步 估算每个组份的参数

假设上一步中得到的 $\gamma(i,k)$ 就是正确的『数据 $x_i$ 由组份 k 生成的概率』，或者，我们可以看做 $x_i$ 其中有 $\gamma(i,k)\times x_i$ 部分是由组份 k 所生成的。

对于所有的数据点，现在实际上可以看作组份 k 生成了 $\{\gamma(i,k)\times x_i \;|\;i=1,2,\dots,N\}$ 这些点。组份 k 是一个标准的高斯分布，所以

$$
\mu_k=\frac{1}{N_k}\sum_{i=1}^{N}\gamma(i,k)x_i \\
\Sigma_k=\frac{1}{N_k}\sum_{i=1}^{N}\gamma(i,k)(x_i-\mu_k)(x_i-\mu_k)^T
$$

## EM 算法的提出

假定有训练集

$$
\{x^{(1)},\dots,x^{(m)}\}
$$

包含 m 个独立样本，希望从众找到该组数据的模型 p(x,z) 的参数，这里 x 就是可以观测到的数据，z 是隐变量。

取对数似然函数

$$
\ell(\theta)=\sum_{i=1}^{m}log\;p(x;\theta)=\sum_{i=1}^{m}log\sum_zp(x,z;\theta)
$$

这里，z 是隐随机变量，直接找到参数的估计是很困难的，我们的策略是建立 $\ell(\theta)$ 的下界，并且求该下界的最大值；重复这个过程，直到收敛到局部最大值

### Jensen 不等式

令 $Q_i$ 是 z 的某一个分布，$Q_i \ge 0$，有

$$
\begin{align*}
\sum_ilog\;p(x^{(i)};\theta) &= \sum_ilog\sum_{z^{(i)}}p(x^{(i)},z^{(i)};\theta) \\
&= \sum_ilog\sum_{z^{(i)}}Q_i(z^{(i)})\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} \\
&\ge \sum_i\sum_{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}
\end{align*}
$$

把 $Q_i(z^{(i)})$ 看成是一个概率，相当于就是求一个期望，于是上面的推导实际上可以看做下面的形式

$$
f(\mathbf{E}x) \le \mathbf{E}f(x)
$$

经过变化之后的式子会稍微好求解一些，上面这个不等式，在满足以下条件时取等号，c 是一个常数

$$
\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}=c
$$

就可以通过这个条件寻找尽量紧的下界，要求是一个常数，也就是说分子分母成比例，有

$$
Q_i(z^{(i)}) \propto p(x^{(i)},z^{(i)};\theta) \;\; and \;\; \sum_z Q_i(z^{(i)})=1
$$

因此可得

$$
\begin{align*}
Q_i(z^{(i)}) &= \frac{p(x^{(i)},z^{(i)};\theta)}{\sum_zp(x^{(i)},z;\theta)} \\
&=\frac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)};\theta)} \\
&=p(z^{(i)}\;|\;x^{(i)};\theta)
\end{align*} 
$$

### EM 算法整体框架

实际上这里的 $Q_i(z^{(i)})$ 就是上面例子的 $\pi_k$

Repeat until convergence {

(E-step) For each $i$, set

$$
Q_i(z^{(i)}) := p(z^{(i)}\;|\;x^{(i)};\theta)
$$

(M-step) Set

$$
\theta := arg max_\theta \sum_i\sum_{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}
$$

}

## 理论框架推导 GMM

随机变量 $X$ 是有 K 个高斯分布混合而成，取各个高斯分布的概率为 $\phi_1,\phi_2,\dots,\phi_K$，第 i 个高斯分布的均值为 $\mu_1$，方差为 $\sum_i$。若观测到随机变量 $X$ 的一系列样本 $x_1,x_2,\dots,x_n$，试估计参数 $\phi,\mu,\Sigma$

### E-step

$$
\omega_j^{(i)}=Q_i(z^{(i)}=j)=P(z^{(i)}=j\;|\;x^{(i)};\phi,\mu,\Sigma)
$$

这里的 $\omega_j^{(i)}$ 表示第 i 个样本来自第 j 个分布的概率

### M-step

将多项分布和高斯分布的参数代入，可得

$$
\begin{align*}
&\sum_{i=1}^m\sum_{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\phi,\mu,\Sigma)}{Q_i(z^{(i)})} \\
=&\sum_{i=1}^m\sum_{j=1}^kQ_i(z^{(i)}=j)log \frac{p(x^{(i)}\;|\;z^{(i)}=j;\mu,\Sigma)p(z^{(i)}=j}{Q_i(z^{(i)}=j)} \\
=&\sum_{i=1}^m\sum_{j=1}^k\omega_j^{(i)}log\frac{\frac{1}{(2\pi)^{m/2}|\Sigma_j^{1/2}|}exp\left(-\frac{1}{2}(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j)\right)·\phi_j}{\omega_j^{(i)}}
\end{align*}
$$

第一次转换是把联合概率转换为条件概率，其中 $z^{(i)}$ 只跟 $\phi$ 有关，$x^{(i)}$ 只跟 $\mu,\Sigma$ 有关，所以可以分开写。于是前面的一个 p 就是一个高斯模型（后面直接代入得到最终公式），后面的一个 p 就是 $\phi_j$ 本身，因此得到这个公式

对均值求偏导

$$
\begin{align*}
&\nabla_{u_l} \sum_{i=1}^m\sum_{j=1}^k\omega_j^{(i)}log\frac{\frac{1}{(2\pi)^{m/2}|\Sigma_j^{1/2}|}exp\left(-\frac{1}{2}(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j)\right)·\phi_j}{\omega_j^{(i)}} \\
=&-\nabla_{u_l} \sum_{i=1}^m\sum_{j=1}^k\omega_j^{(i)}\frac{1}{2}(x^{(i)}-\mu_j)^T\Sigma_j^{-1}(x^{(i)}-\mu_j) \\
=&\frac{1}{2}\sum_{i=1}^{m}\omega_j^{(i)}\nabla_{u_l}2u_l^T\Sigma_j^{-1}x^{(i)}-u_l^T\Sigma_j^{-1}u_l \\
=&\sum_{i=1}^m\omega_j^{(i)}\left(\Sigma_j^{-1}x^{(i)}-\Sigma_j^{-1}u_l\right)
\end{align*}
$$

令上式等于 0，解得均值

$$
u_l := \frac{\sum_{i=1}^{m}\omega_l^{(i)}x^{(i)}}{\sum_{i=1}^{m}\omega_l^{(i)}}
$$

同样的方法对 $\Sigma$ 求偏导，并令其等于 0，解得

$$
\Sigma_j=\frac{\sum_{i=1}^{m}\omega_l^{(i)}(x^{(i)}-\mu_j)(x^{(i)}-\mu_j)^T}{\sum_{i=1}^{m}\omega_l^{(i)}}
$$

关于 $\phi$ ，在目标函数与它相关的是

$$
\sum_{i=1}^m\sum_{j=1}^k\omega_j^{(i)}log\phi_j
$$

由于多项分布的概率和为 1，建立拉格朗日方程

$$
L(\phi)=\sum_{i=1}^m\sum_{j=1}^k\omega_j^{(i)}log\phi_j+\beta\left(\sum_{j=1}^k\phi_j-1\right)
$$

这样求解的 $\phi_i$ 一定非负，所以不用考虑这个条件，然后对 $\phi,\beta$ 分别求偏导，令其为 0，可得

$$
\frac{\partial}{\partial\phi_j}L(\phi)=\sum_{i=1}^{m}\frac{\omega_j^{(i)}}{\phi_j}+\beta \\
-\beta = \sum_{i=1}^m\sum_{j=1}^k\omega_j^{(i)}=\sum_{i=1}^m1=m \\
\phi_j:=\frac{1}{m}\sum_{i=1}^{m}\omega_j^{(i)}
$$

估算出来这三个参数，再带入到 E-step，就可以循环进行计算了

